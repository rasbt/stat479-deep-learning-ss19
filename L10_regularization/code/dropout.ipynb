{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 479: Deep Learning (Spring 2019)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat479-ss2019/  \n",
    "GitHub repository: https://github.com/rasbt/stat479-deep-learning-ss19\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "RANDOM_SEED = 1\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(net, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.view(-1, 28*28).to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            a1, a2 = net.forward(features)\n",
    "            predicted_labels = torch.argmax(a2, 1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (predicted_labels == targets).sum()\n",
    "        return correct_pred.float()/num_examples * 100\n",
    "\n",
    "\n",
    "def compute_loss(net, data_loader):\n",
    "    curr_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for cnt, (features, targets) in enumerate(data_loader):\n",
    "            features = features.view(-1, 28*28).to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            logits, probas = net.forward(features)\n",
    "            loss = F.nll_loss(torch.log(probas), targets)\n",
    "            # or better (more numerically stable):\n",
    "            # loss = F.cross_entropy(logits, targets)\n",
    "            # see \n",
    "            # ../../other/pytorch-lossfunc-cheatsheet.md\n",
    "            curr_loss += loss\n",
    "        return float(curr_loss)/cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object-Oriented Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes, drop_proba, \n",
    "                 num_hidden_1, num_hidden_2):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        \n",
    "        self.my_network = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_features, num_hidden_1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(drop_proba),\n",
    "            torch.nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(drop_proba),\n",
    "            torch.nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "           \n",
    "    def forward(self, x):\n",
    "        logits = self.my_network(x)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/469 | Cost: 2.3018\n",
      "Epoch: 001/010 | Batch 050/469 | Cost: 2.1577\n",
      "Epoch: 001/010 | Batch 100/469 | Cost: 1.5877\n",
      "Epoch: 001/010 | Batch 150/469 | Cost: 1.0578\n",
      "Epoch: 001/010 | Batch 200/469 | Cost: 0.9922\n",
      "Epoch: 001/010 | Batch 250/469 | Cost: 0.9561\n",
      "Epoch: 001/010 | Batch 300/469 | Cost: 0.7748\n",
      "Epoch: 001/010 | Batch 350/469 | Cost: 0.6270\n",
      "Epoch: 001/010 | Batch 400/469 | Cost: 0.6704\n",
      "Epoch: 001/010 | Batch 450/469 | Cost: 0.6320\n",
      "Epoch: 001/010 Train Cost: 0.3947\n",
      "Time elapsed: 0.23 min\n",
      "Epoch: 002/010 | Batch 000/469 | Cost: 0.5669\n",
      "Epoch: 002/010 | Batch 050/469 | Cost: 0.7065\n",
      "Epoch: 002/010 | Batch 100/469 | Cost: 0.3853\n",
      "Epoch: 002/010 | Batch 150/469 | Cost: 0.4498\n",
      "Epoch: 002/010 | Batch 200/469 | Cost: 0.6832\n",
      "Epoch: 002/010 | Batch 250/469 | Cost: 0.5445\n",
      "Epoch: 002/010 | Batch 300/469 | Cost: 0.4575\n",
      "Epoch: 002/010 | Batch 350/469 | Cost: 0.4322\n",
      "Epoch: 002/010 | Batch 400/469 | Cost: 0.3241\n",
      "Epoch: 002/010 | Batch 450/469 | Cost: 0.4325\n",
      "Epoch: 002/010 Train Cost: 0.2746\n",
      "Time elapsed: 0.44 min\n",
      "Epoch: 003/010 | Batch 000/469 | Cost: 0.3168\n",
      "Epoch: 003/010 | Batch 050/469 | Cost: 0.4063\n",
      "Epoch: 003/010 | Batch 100/469 | Cost: 0.3384\n",
      "Epoch: 003/010 | Batch 150/469 | Cost: 0.4642\n",
      "Epoch: 003/010 | Batch 200/469 | Cost: 0.4693\n",
      "Epoch: 003/010 | Batch 250/469 | Cost: 0.4285\n",
      "Epoch: 003/010 | Batch 300/469 | Cost: 0.3663\n",
      "Epoch: 003/010 | Batch 350/469 | Cost: 0.5323\n",
      "Epoch: 003/010 | Batch 400/469 | Cost: 0.3879\n",
      "Epoch: 003/010 | Batch 450/469 | Cost: 0.4234\n",
      "Epoch: 003/010 Train Cost: 0.2229\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 004/010 | Batch 000/469 | Cost: 0.4573\n",
      "Epoch: 004/010 | Batch 050/469 | Cost: 0.3627\n",
      "Epoch: 004/010 | Batch 100/469 | Cost: 0.4336\n",
      "Epoch: 004/010 | Batch 150/469 | Cost: 0.4301\n",
      "Epoch: 004/010 | Batch 200/469 | Cost: 0.3651\n",
      "Epoch: 004/010 | Batch 250/469 | Cost: 0.5661\n",
      "Epoch: 004/010 | Batch 300/469 | Cost: 0.2232\n",
      "Epoch: 004/010 | Batch 350/469 | Cost: 0.5388\n",
      "Epoch: 004/010 | Batch 400/469 | Cost: 0.4334\n",
      "Epoch: 004/010 | Batch 450/469 | Cost: 0.4903\n",
      "Epoch: 004/010 Train Cost: 0.1953\n",
      "Time elapsed: 0.85 min\n",
      "Epoch: 005/010 | Batch 000/469 | Cost: 0.3371\n",
      "Epoch: 005/010 | Batch 050/469 | Cost: 0.3808\n",
      "Epoch: 005/010 | Batch 100/469 | Cost: 0.3904\n",
      "Epoch: 005/010 | Batch 150/469 | Cost: 0.2885\n",
      "Epoch: 005/010 | Batch 200/469 | Cost: 0.2006\n",
      "Epoch: 005/010 | Batch 250/469 | Cost: 0.2582\n",
      "Epoch: 005/010 | Batch 300/469 | Cost: 0.2705\n",
      "Epoch: 005/010 | Batch 350/469 | Cost: 0.3636\n",
      "Epoch: 005/010 | Batch 400/469 | Cost: 0.4061\n",
      "Epoch: 005/010 | Batch 450/469 | Cost: 0.3334\n",
      "Epoch: 005/010 Train Cost: 0.1694\n",
      "Time elapsed: 1.06 min\n",
      "Epoch: 006/010 | Batch 000/469 | Cost: 0.3828\n",
      "Epoch: 006/010 | Batch 050/469 | Cost: 0.2656\n",
      "Epoch: 006/010 | Batch 100/469 | Cost: 0.3944\n",
      "Epoch: 006/010 | Batch 150/469 | Cost: 0.3371\n",
      "Epoch: 006/010 | Batch 200/469 | Cost: 0.2844\n",
      "Epoch: 006/010 | Batch 250/469 | Cost: 0.2094\n",
      "Epoch: 006/010 | Batch 300/469 | Cost: 0.3925\n",
      "Epoch: 006/010 | Batch 350/469 | Cost: 0.3402\n",
      "Epoch: 006/010 | Batch 400/469 | Cost: 0.1937\n",
      "Epoch: 006/010 | Batch 450/469 | Cost: 0.2290\n",
      "Epoch: 006/010 Train Cost: 0.1553\n",
      "Time elapsed: 1.27 min\n",
      "Epoch: 007/010 | Batch 000/469 | Cost: 0.3611\n",
      "Epoch: 007/010 | Batch 050/469 | Cost: 0.2167\n",
      "Epoch: 007/010 | Batch 100/469 | Cost: 0.2445\n",
      "Epoch: 007/010 | Batch 150/469 | Cost: 0.1730\n",
      "Epoch: 007/010 | Batch 200/469 | Cost: 0.3760\n",
      "Epoch: 007/010 | Batch 250/469 | Cost: 0.3117\n",
      "Epoch: 007/010 | Batch 300/469 | Cost: 0.2168\n",
      "Epoch: 007/010 | Batch 350/469 | Cost: 0.3025\n",
      "Epoch: 007/010 | Batch 400/469 | Cost: 0.2779\n",
      "Epoch: 007/010 | Batch 450/469 | Cost: 0.3534\n",
      "Epoch: 007/010 Train Cost: 0.1448\n",
      "Time elapsed: 1.48 min\n",
      "Epoch: 008/010 | Batch 000/469 | Cost: 0.3840\n",
      "Epoch: 008/010 | Batch 050/469 | Cost: 0.2479\n",
      "Epoch: 008/010 | Batch 100/469 | Cost: 0.3104\n",
      "Epoch: 008/010 | Batch 150/469 | Cost: 0.2598\n",
      "Epoch: 008/010 | Batch 200/469 | Cost: 0.3078\n",
      "Epoch: 008/010 | Batch 250/469 | Cost: 0.3695\n",
      "Epoch: 008/010 | Batch 300/469 | Cost: 0.3032\n",
      "Epoch: 008/010 | Batch 350/469 | Cost: 0.5036\n",
      "Epoch: 008/010 | Batch 400/469 | Cost: 0.3004\n",
      "Epoch: 008/010 | Batch 450/469 | Cost: 0.2090\n",
      "Epoch: 008/010 Train Cost: 0.1368\n",
      "Time elapsed: 1.69 min\n",
      "Epoch: 009/010 | Batch 000/469 | Cost: 0.2249\n",
      "Epoch: 009/010 | Batch 050/469 | Cost: 0.2763\n",
      "Epoch: 009/010 | Batch 100/469 | Cost: 0.2637\n",
      "Epoch: 009/010 | Batch 150/469 | Cost: 0.2580\n",
      "Epoch: 009/010 | Batch 200/469 | Cost: 0.3750\n",
      "Epoch: 009/010 | Batch 250/469 | Cost: 0.1523\n",
      "Epoch: 009/010 | Batch 300/469 | Cost: 0.3013\n",
      "Epoch: 009/010 | Batch 350/469 | Cost: 0.3070\n",
      "Epoch: 009/010 | Batch 400/469 | Cost: 0.2615\n",
      "Epoch: 009/010 | Batch 450/469 | Cost: 0.3411\n",
      "Epoch: 009/010 Train Cost: 0.1352\n",
      "Time elapsed: 1.90 min\n",
      "Epoch: 010/010 | Batch 000/469 | Cost: 0.2861\n",
      "Epoch: 010/010 | Batch 050/469 | Cost: 0.3736\n",
      "Epoch: 010/010 | Batch 100/469 | Cost: 0.2830\n",
      "Epoch: 010/010 | Batch 150/469 | Cost: 0.1833\n",
      "Epoch: 010/010 | Batch 200/469 | Cost: 0.2958\n",
      "Epoch: 010/010 | Batch 250/469 | Cost: 0.2259\n",
      "Epoch: 010/010 | Batch 300/469 | Cost: 0.1707\n",
      "Epoch: 010/010 | Batch 350/469 | Cost: 0.1545\n",
      "Epoch: 010/010 | Batch 400/469 | Cost: 0.2858\n",
      "Epoch: 010/010 | Batch 450/469 | Cost: 0.3356\n",
      "Epoch: 010/010 Train Cost: 0.1216\n",
      "Time elapsed: 2.11 min\n",
      "Total Training Time: 2.11 min\n",
      "Training Accuracy: 96.60\n",
      "Test Accuracy: 96.21\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "### Model Initialization\n",
    "#################################\n",
    "    \n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = MultilayerPerceptron(num_features=28*28,\n",
    "                             num_hidden_1=100,\n",
    "                             num_hidden_2=50,\n",
    "                             drop_proba=0.5,\n",
    "                             num_classes=10)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#################################\n",
    "### Training\n",
    "#################################\n",
    "\n",
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.view(-1, 28*28).to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        \n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Cost: %.4f' % (\n",
    "                epoch+1, NUM_EPOCHS, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Accuracy: %.2f' % compute_accuracy(model, train_loader))\n",
    "    print('Test Accuracy: %.2f' % compute_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes, drop_proba, \n",
    "                 num_hidden_1, num_hidden_2):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        \n",
    "        self.drop_proba = drop_proba\n",
    "        self.linear_1 = torch.nn.Linear(num_features,\n",
    "                                        num_hidden_1)\n",
    "\n",
    "        self.linear_2 = torch.nn.Linear(num_hidden_1,\n",
    "                                        num_hidden_2)\n",
    "\n",
    "        self.linear_out = torch.nn.Linear(num_hidden_2,\n",
    "                                          num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear_1(x)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=self.drop_proba, training=self.training)\n",
    "        out = self.linear_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = F.dropout(out, p=self.drop_proba, training=self.training)\n",
    "        logits = self.linear_out(out)\n",
    "        probas = F.log_softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 000/469 | Cost: 2.3018\n",
      "Epoch: 001/010 | Batch 050/469 | Cost: 2.1577\n",
      "Epoch: 001/010 | Batch 100/469 | Cost: 1.5877\n",
      "Epoch: 001/010 | Batch 150/469 | Cost: 1.0578\n",
      "Epoch: 001/010 | Batch 200/469 | Cost: 0.9922\n",
      "Epoch: 001/010 | Batch 250/469 | Cost: 0.9561\n",
      "Epoch: 001/010 | Batch 300/469 | Cost: 0.7748\n",
      "Epoch: 001/010 | Batch 350/469 | Cost: 0.6270\n",
      "Epoch: 001/010 | Batch 400/469 | Cost: 0.6704\n",
      "Epoch: 001/010 | Batch 450/469 | Cost: 0.6320\n",
      "Epoch: 001/010 Train Cost: nan\n",
      "Time elapsed: 0.20 min\n",
      "Epoch: 002/010 | Batch 000/469 | Cost: 0.5669\n",
      "Epoch: 002/010 | Batch 050/469 | Cost: 0.7065\n",
      "Epoch: 002/010 | Batch 100/469 | Cost: 0.3853\n",
      "Epoch: 002/010 | Batch 150/469 | Cost: 0.4498\n",
      "Epoch: 002/010 | Batch 200/469 | Cost: 0.6832\n",
      "Epoch: 002/010 | Batch 250/469 | Cost: 0.5445\n",
      "Epoch: 002/010 | Batch 300/469 | Cost: 0.4575\n",
      "Epoch: 002/010 | Batch 350/469 | Cost: 0.4322\n",
      "Epoch: 002/010 | Batch 400/469 | Cost: 0.3241\n",
      "Epoch: 002/010 | Batch 450/469 | Cost: 0.4325\n",
      "Epoch: 002/010 Train Cost: nan\n",
      "Time elapsed: 0.41 min\n",
      "Epoch: 003/010 | Batch 000/469 | Cost: 0.3168\n",
      "Epoch: 003/010 | Batch 050/469 | Cost: 0.4063\n",
      "Epoch: 003/010 | Batch 100/469 | Cost: 0.3384\n",
      "Epoch: 003/010 | Batch 150/469 | Cost: 0.4642\n",
      "Epoch: 003/010 | Batch 200/469 | Cost: 0.4693\n",
      "Epoch: 003/010 | Batch 250/469 | Cost: 0.4285\n",
      "Epoch: 003/010 | Batch 300/469 | Cost: 0.3663\n",
      "Epoch: 003/010 | Batch 350/469 | Cost: 0.5323\n",
      "Epoch: 003/010 | Batch 400/469 | Cost: 0.3879\n",
      "Epoch: 003/010 | Batch 450/469 | Cost: 0.4234\n",
      "Epoch: 003/010 Train Cost: nan\n",
      "Time elapsed: 0.63 min\n",
      "Epoch: 004/010 | Batch 000/469 | Cost: 0.4573\n",
      "Epoch: 004/010 | Batch 050/469 | Cost: 0.3627\n",
      "Epoch: 004/010 | Batch 100/469 | Cost: 0.4336\n",
      "Epoch: 004/010 | Batch 150/469 | Cost: 0.4301\n",
      "Epoch: 004/010 | Batch 200/469 | Cost: 0.3651\n",
      "Epoch: 004/010 | Batch 250/469 | Cost: 0.5661\n",
      "Epoch: 004/010 | Batch 300/469 | Cost: 0.2232\n",
      "Epoch: 004/010 | Batch 350/469 | Cost: 0.5388\n",
      "Epoch: 004/010 | Batch 400/469 | Cost: 0.4334\n",
      "Epoch: 004/010 | Batch 450/469 | Cost: 0.4903\n",
      "Epoch: 004/010 Train Cost: nan\n",
      "Time elapsed: 0.84 min\n",
      "Epoch: 005/010 | Batch 000/469 | Cost: 0.3371\n",
      "Epoch: 005/010 | Batch 050/469 | Cost: 0.3808\n",
      "Epoch: 005/010 | Batch 100/469 | Cost: 0.3904\n",
      "Epoch: 005/010 | Batch 150/469 | Cost: 0.2885\n",
      "Epoch: 005/010 | Batch 200/469 | Cost: 0.2006\n",
      "Epoch: 005/010 | Batch 250/469 | Cost: 0.2582\n",
      "Epoch: 005/010 | Batch 300/469 | Cost: 0.2705\n",
      "Epoch: 005/010 | Batch 350/469 | Cost: 0.3636\n",
      "Epoch: 005/010 | Batch 400/469 | Cost: 0.4061\n",
      "Epoch: 005/010 | Batch 450/469 | Cost: 0.3334\n",
      "Epoch: 005/010 Train Cost: nan\n",
      "Time elapsed: 1.04 min\n",
      "Epoch: 006/010 | Batch 000/469 | Cost: 0.3828\n",
      "Epoch: 006/010 | Batch 050/469 | Cost: 0.2656\n",
      "Epoch: 006/010 | Batch 100/469 | Cost: 0.3944\n",
      "Epoch: 006/010 | Batch 150/469 | Cost: 0.3371\n",
      "Epoch: 006/010 | Batch 200/469 | Cost: 0.2844\n",
      "Epoch: 006/010 | Batch 250/469 | Cost: 0.2094\n",
      "Epoch: 006/010 | Batch 300/469 | Cost: 0.3925\n",
      "Epoch: 006/010 | Batch 350/469 | Cost: 0.3402\n",
      "Epoch: 006/010 | Batch 400/469 | Cost: 0.1937\n",
      "Epoch: 006/010 | Batch 450/469 | Cost: 0.2290\n",
      "Epoch: 006/010 Train Cost: nan\n",
      "Time elapsed: 1.25 min\n",
      "Epoch: 007/010 | Batch 000/469 | Cost: 0.3611\n",
      "Epoch: 007/010 | Batch 050/469 | Cost: 0.2167\n",
      "Epoch: 007/010 | Batch 100/469 | Cost: 0.2445\n",
      "Epoch: 007/010 | Batch 150/469 | Cost: 0.1730\n",
      "Epoch: 007/010 | Batch 200/469 | Cost: 0.3760\n",
      "Epoch: 007/010 | Batch 250/469 | Cost: 0.3117\n",
      "Epoch: 007/010 | Batch 300/469 | Cost: 0.2168\n",
      "Epoch: 007/010 | Batch 350/469 | Cost: 0.3025\n",
      "Epoch: 007/010 | Batch 400/469 | Cost: 0.2779\n",
      "Epoch: 007/010 | Batch 450/469 | Cost: 0.3534\n",
      "Epoch: 007/010 Train Cost: nan\n",
      "Time elapsed: 1.46 min\n",
      "Epoch: 008/010 | Batch 000/469 | Cost: 0.3840\n",
      "Epoch: 008/010 | Batch 050/469 | Cost: 0.2479\n",
      "Epoch: 008/010 | Batch 100/469 | Cost: 0.3104\n",
      "Epoch: 008/010 | Batch 150/469 | Cost: 0.2598\n",
      "Epoch: 008/010 | Batch 200/469 | Cost: 0.3078\n",
      "Epoch: 008/010 | Batch 250/469 | Cost: 0.3695\n",
      "Epoch: 008/010 | Batch 300/469 | Cost: 0.3032\n",
      "Epoch: 008/010 | Batch 350/469 | Cost: 0.5036\n",
      "Epoch: 008/010 | Batch 400/469 | Cost: 0.3004\n",
      "Epoch: 008/010 | Batch 450/469 | Cost: 0.2090\n",
      "Epoch: 008/010 Train Cost: nan\n",
      "Time elapsed: 1.68 min\n",
      "Epoch: 009/010 | Batch 000/469 | Cost: 0.2249\n",
      "Epoch: 009/010 | Batch 050/469 | Cost: 0.2763\n",
      "Epoch: 009/010 | Batch 100/469 | Cost: 0.2637\n",
      "Epoch: 009/010 | Batch 150/469 | Cost: 0.2580\n",
      "Epoch: 009/010 | Batch 200/469 | Cost: 0.3750\n",
      "Epoch: 009/010 | Batch 250/469 | Cost: 0.1523\n",
      "Epoch: 009/010 | Batch 300/469 | Cost: 0.3013\n",
      "Epoch: 009/010 | Batch 350/469 | Cost: 0.3070\n",
      "Epoch: 009/010 | Batch 400/469 | Cost: 0.2615\n",
      "Epoch: 009/010 | Batch 450/469 | Cost: 0.3411\n",
      "Epoch: 009/010 Train Cost: nan\n",
      "Time elapsed: 1.90 min\n",
      "Epoch: 010/010 | Batch 000/469 | Cost: 0.2861\n",
      "Epoch: 010/010 | Batch 050/469 | Cost: 0.3736\n",
      "Epoch: 010/010 | Batch 100/469 | Cost: 0.2830\n",
      "Epoch: 010/010 | Batch 150/469 | Cost: 0.1833\n",
      "Epoch: 010/010 | Batch 200/469 | Cost: 0.2958\n",
      "Epoch: 010/010 | Batch 250/469 | Cost: 0.2259\n",
      "Epoch: 010/010 | Batch 300/469 | Cost: 0.1707\n",
      "Epoch: 010/010 | Batch 350/469 | Cost: 0.1545\n",
      "Epoch: 010/010 | Batch 400/469 | Cost: 0.2858\n",
      "Epoch: 010/010 | Batch 450/469 | Cost: 0.3356\n",
      "Epoch: 010/010 Train Cost: nan\n",
      "Time elapsed: 2.12 min\n",
      "Total Training Time: 2.12 min\n",
      "Training Accuracy: 96.60\n",
      "Test Accuracy: 96.21\n"
     ]
    }
   ],
   "source": [
    "#################################\n",
    "### Model Initialization\n",
    "#################################\n",
    "    \n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "model = MultilayerPerceptron(num_features=28*28,\n",
    "                             num_hidden_1=100,\n",
    "                             num_hidden_2=50,\n",
    "                             drop_proba=0.5,\n",
    "                             num_classes=10)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "#################################\n",
    "### Training\n",
    "#################################\n",
    "\n",
    "start_time = time.time()\n",
    "minibatch_cost = []\n",
    "epoch_cost = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.view(-1, 28*28).to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        \n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        minibatch_cost.append(cost)\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "       \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cost = compute_loss(model, train_loader)\n",
    "        epoch_cost.append(cost)\n",
    "        print('Epoch: %03d/%03d Train Cost: %.4f' % (\n",
    "                epoch+1, NUM_EPOCHS, cost))\n",
    "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "    print('Training Accuracy: %.2f' % compute_accuracy(model, train_loader))\n",
    "    print('Test Accuracy: %.2f' % compute_accuracy(model, test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchvision 0.2.1\n",
      "torch       1.0.1\n",
      "Sebastian Raschka\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -iv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
